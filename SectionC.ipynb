{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C (KNN Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors (KNN)\n",
    "\n",
    "### Introduction\n",
    "K-Nearest Neighbors（KNN）is a supervised machine learning algorithm that can be used to solve both classification and regression problems. It is based on the assumption that data points will fall into the vicinity of the same class of data. Simply put, similar things are close to each other. \n",
    "\n",
    "KNN is a lazy algorithmn, because it doesn’t learn from the training dataset but “memorises” the training dataset instead, like putting labled data in some metric space. so it does not need time to train the model compared to other machine learning model like logistic regression. However, when we predict new data using KNN, it will take much time to search for the nearest neighbors in the entire training dataset, calulating the distance and sorting them.\n",
    "\n",
    "### How KNN works(Classification)\n",
    "KNN algorithms can be summarised in the following steps:\n",
    "\n",
    "1. Select the number k of the nearest neighbours\n",
    "2. Calculate the distance between new data point and each of the training data.\n",
    "3. Find the k nearest neighbours from the new data\n",
    "4. Take the most frequent of labels as the prediction  of the label of new data point\n",
    "\n",
    "KNN algorithm can be used for both classification and regression problems. \n",
    "\n",
    "### How KNN predits\n",
    "For a given new data point, find the k nearest neighbours from new data point in traning dataset, collect neighbours' labels, finally, take the mode of the labels as the prediction for the label of new data point.\n",
    "\n",
    "### KNN Hyperparameters\n",
    "Hyperparameters are the parameters to control the model learning process. We can tune these hyperparameters to get a model with optimal performance.\n",
    "Here are part of hyperparmeters that I used in the following task.\n",
    "\n",
    "- Number of neighbors(K):the number of nearest neighbors\n",
    "- Weights: When calculating the distance, we can choose 'uniform’ weights that all points in each neighborhood are weighted equally or\n",
    "‘distance’ weight points by the inverse of their distance.\n",
    "- p: This determines that we can use what kind of metric to calculate distance between neighbours(Euclidean Distance, Manhattan Distance, and Minkowski Distance) When p = 1, use manhattan_distance, and euclidean_distance  p = 2. For arbitrary p, minkowski_distance is used.\n",
    "\n",
    "### Application\n",
    "KNN can be used in both solve both classification and regression problems, when using KNN to solve regression problem, it will use the average of target value of K nearst neighbors as the prediction value of the new data, when using to solve classification problem, it will use the mode as the prediction label of new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Here is the introduction of the dataset. \n",
    "\n",
    "This dataset is about 11 clinical features for predicting heart disease events, from Kaggle [Heart Failure Prediction Dataset](https://www.kaggle.com/fedesoriano/heart-failure-prediction/)\n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "- Age: (Discreate variable)    age of the patient [years]\n",
    "- Sex: (Binary variable)   sex of the patient [M: Male, F: Female]\n",
    "- ChestPainType: (Categorical variable) chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "- RestingBP:(Continuous variable)  resting blood pressure [mm Hg]\n",
    "- Cholesterol:(Continuous variable)    serum cholesterol [mm/dl]\n",
    "- FastingBS: (Binary variable)     fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "- RestingECG: (Categorical variable)   resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality, LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "- MaxHR:(Continuos variable)   maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "- ExerciseAngina: (Binary variable)    exercise-induced angina [Y: Yes, N: No]\n",
    "- Oldpeak: (Continuous variable)   Numeric value measured in depression\n",
    "- ST_Slope: (Categorical variable)     he slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "- HeartDisease:(Binary variable)   output class [1: heart disease, 0: Normal]\n",
    "\n",
    "The classifaction goal is to predict heart falure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"heart.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 918 observations and 12 features (11 independent features and 1 dependent feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in these dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age               0\n",
      "Sex               0\n",
      "ChestPainType     0\n",
      "RestingBP         0\n",
      "Cholesterol       0\n",
      "FastingBS         0\n",
      "RestingECG        0\n",
      "MaxHR             0\n",
      "ExerciseAngina    0\n",
      "Oldpeak           0\n",
      "ST_Slope          0\n",
      "HeartDisease      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target class distribution is fiarly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.553377\n",
       "0    0.446623\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of target class\n",
    "df[\"HeartDisease\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into attributes (X) and labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical values\n",
    "The dataset contains categorical variables like 'ChestPainType'\n",
    "Since meachine learning algorithms cannot work with categorical data directly, we need to convert the categorical values into numeric values by using one-hot code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RestingBP  Cholesterol  FastingBS  MaxHR  Oldpeak  Sex_F  Sex_M  \\\n",
       "0   40        140          289          0    172      0.0      0      1   \n",
       "1   49        160          180          0    156      1.0      1      0   \n",
       "2   37        130          283          0     98      0.0      0      1   \n",
       "3   48        138          214          0    108      1.5      1      0   \n",
       "4   54        150          195          0    122      0.0      0      1   \n",
       "\n",
       "   ChestPainType_ASY  ChestPainType_ATA  ChestPainType_NAP  ChestPainType_TA  \\\n",
       "0                  0                  1                  0                 0   \n",
       "1                  0                  0                  1                 0   \n",
       "2                  0                  1                  0                 0   \n",
       "3                  1                  0                  0                 0   \n",
       "4                  0                  0                  1                 0   \n",
       "\n",
       "   RestingECG_LVH  RestingECG_Normal  RestingECG_ST  ExerciseAngina_N  \\\n",
       "0               0                  1              0                 1   \n",
       "1               0                  1              0                 1   \n",
       "2               0                  0              1                 1   \n",
       "3               0                  1              0                 0   \n",
       "4               0                  1              0                 1   \n",
       "\n",
       "   ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \n",
       "0                 0              0              0            1  \n",
       "1                 0              0              1            0  \n",
       "2                 0              0              0            1  \n",
       "3                 1              0              1            0  \n",
       "4                 0              0              0            1  "
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying One hot Encoding on the Categorical columns\n",
    "X=pd.get_dummies(X)\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don’t want the model to over-learn from training data and perform poorly after being tested in test data. we need to assess how well the model is generalizing. Hence, so we  separate input data set into training, validation, and testing subsets to prevent the model from overfitting and to evaluate the model effectively.\n",
    "\n",
    "Train dataset is used to fit the parameters to the model, or 'finding nearest neighbors'\n",
    "\n",
    "Valid dataset is used to assess the model's performance and avoid overfitting (the model is really good at classifying the samples in the training dataset but cannot generalize and make accurate classifications on the unseen data), it helps us tune the model's hyperparameters, so we can improve the model according to the results on validation.\n",
    "\n",
    "test dataset is used to assess the performance of the final chosen model and it is equivalent to the future unseen data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset \n",
    "we split dataset into train dataset, validation dataset, and test dataset in the ratio 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train dataset 80%, valid dataset 10%, test dataset 10%\n",
    "# split the data in training and remaining dataset, and split the remaining dataset\n",
    "X_remain, X_test, y_remain, y_test = train_test_split(X, y, test_size = 0.1, random_state = 3)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_remain, y_remain, test_size = 1/9,random_state = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "Choosing the right metric is crucial while evaluating machine learning models. \n",
    "\n",
    "Accuracy is an intuitive metric to assess the perfomance of model, and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.\n",
    "\n",
    "accuracy is applied in when the target class distribution is fairly balanced and we do not prefer any type of class. So I decide to use 'accuracy_score' function for this classication task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train KNN model\n",
    "\n",
    "I use 'KNeighborsClassifier' function form 'sklearn' library to generate KNN model.\n",
    "First, I use train dataset to fit the KNN model and iterate the hyperparameters that we want to tuning (p, k, weigth) and in each for loop tries a combination of hyperparameters. The model tuning the hyperparameters based on it's perfomance (assessed by classification accuracy score) on validation data set, and finally we record the best hyperparameters for the training model.\n",
    "\n",
    "### Hyperparameters tuning\n",
    "p parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance, and euclidean_distance for p = 2. For arbitrary p (eg. p=3), minkowski_distance is used.\n",
    "\n",
    "weights{‘uniform’, ‘distance’}\n",
    "Weight function used in prediction. Possible values:\n",
    "‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_p= 1\n",
      "best k= 25\n",
      "best weight= uniform\n",
      "best score= 0.7934782608695652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "best_p = 1\n",
    "best_score = 0.0\n",
    "best_k = 1\n",
    "weight_option = [\"uniform\", \"distance\"]\n",
    "k_range = list(range(1,31))\n",
    "p_options = [1,2,3]\n",
    "\n",
    "for p_o in p_options:\n",
    "    for k in k_range:\n",
    "        for weight in weight_option:\n",
    "            knn = KNeighborsClassifier(n_neighbors = k, weights = weight, p = p_o)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_valid_pred = knn.predict(X_valid)\n",
    "            score = knn.score(X_valid, y_valid) # Return the mean accuracy on the validate dataset\n",
    "            if score > best_score:\n",
    "                best_p = p_o\n",
    "                best_k = k\n",
    "                best_weight = weight\n",
    "                best_score = score\n",
    "                \n",
    "print(\"best_p=\", best_p)\n",
    "print(\"best k=\", best_k)\n",
    "print(\"best weight=\", best_weight)\n",
    "print(\"best score=\", best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test KNN model on test dataset\n",
    "\n",
    "I get 86.96% prediction accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.8696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 25, p = 1, weights= 'uniform')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred=knn.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.4f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Now, let's try use cross-validation to tune the hyperparameters.\n",
    "\n",
    "There are a lot of different techniques that may be used to cross-validate a model. In this task, I choose K-folds validation to conduct cross-validation and set k = 5, here is the how it works.\n",
    "\n",
    "Dataset is randomly split up into 'k' folds. One of the folds is validation set and the rest are training set. The model is trained on the training set and scored on the validation set to find best hyperparameters. Then the process is repeated until each group has been used as the validation set.\n",
    "\n",
    "### Train KNN_2 model\n",
    "\n",
    "I use 'GridSearchCV' function from sklearn to find the best set of hyperparameters that I tuning. In the grid search method, I create a grid of possible values for hyperparameters. Each iteration tries a combination of hyperparameters in a specific order. It fits the model on each and every combination of hyperparameter possible and records the model performance. Finally, it returns the best KNN model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters {'n_neighbors': 17, 'p': 1, 'weights': 'distance'}\n",
      "accuracy_score : 0.7748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters that we want to tune.\n",
    "k_range = list(range(1,31))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "p_options=[1,2,3]\n",
    "\n",
    "param_dict = dict(n_neighbors = k_range, weights = weight_options, p = p_options)\n",
    "\n",
    "#Create KNN classiifier\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use 5 fold cross-validation, use accuracy to evaluate the performance of the model on the validation set\n",
    "grid = GridSearchCV(knn_2, param_grid = param_dict, scoring = 'accuracy', cv = 5) \n",
    "\n",
    "#Fit the model (remain dataset includes train and validation dataset that I split before.)\n",
    "best_model = grid.fit(X_remain,y_remain) \n",
    "\n",
    "#Print the value of best hyperparameters and best accuracy score\n",
    "print('best hyperparameters', best_model.best_params_)\n",
    "print('accuracy_score : %.4f' % best_model.best_score_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test KNN_2 model on test dataset\n",
    "\n",
    "After tuning the hyperparameters by crossvalidation, I get another model with different hyperparameters, then I test the best performance model on test dataset and get 88.04% prediction accuracy, which is better than before(86.96%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.8804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_2 = KNeighborsClassifier(n_neighbors = 17, p = 1, weights= 'distance')\n",
    "\n",
    "knn_2.fit(X_remain, y_remain)\n",
    "\n",
    "y_test_pred=knn_2.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.4f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model that trained by cross validation has a higher accuarcy score on the same test dataset. \n",
    "I think that is because when I split the origin dataset into three sets, which reduce the number of data in training date which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets. But when useing cross validation, we can use of training data more effiently as every observation is used for both training and validating, thus get more accurate estimate of unseen data. \n",
    "However, it takes 13.8s to run cross-validation and it takes only 3.2 to run on one validation dataset, because K-fold cross-validation repeats the train/test split K-times, so it need more time to train the model and tune hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d4b19282c4f1a35886decbca8caed4edeb52dcc9663281933ed3bb3d17a686d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
