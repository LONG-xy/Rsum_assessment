{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C (KNN Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors (KNN)\n",
    "### Introduction\n",
    "K-Nearest Neighbors（KNN）is a supervised machine learning algorithm that can be used to solve both classification and regression problems.  It is also one of the best-known classification algorithms. The principle is that known data are arranged in a space defined by the selected features. When a new data is supplied to the algorithm, the algorithm will compare the classes of the k closest data to determine the class of the new data. It is based on the assumption that data points will fall into the vicinity of the same class of data. Simply put, similar things are close to each other. KNN is a lazy algorithm, because it doesn’t learn from the training dataset but “memorises” the training dataset instead, like putting labeled data in some metric space. KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).\n",
    "\n",
    "\n",
    "## Pros of K Nearest Neighbors\n",
    "1. KNN is simple and intuive\n",
    "KNN is a simple algorithm and hence easy to interpret the prediction.\n",
    "2. KNN makes no assumptions about input data \n",
    "Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure is determined from the dataset. This will be very helpful in practice where most of the real-world datasets do not follow mathematical theoretical assumptions. e.g., in Linear Regression we assume the linear dependency of labels on features.\n",
    "it is a non-parametric model so makes no assumption about the underlying data pattern.Therefore, k-Nearest Neighbors is often the first option when there is a little or no prior knowledge about the data distribution.\n",
    "3. KNN does not need the training step\n",
    "The training phase of the algorithm consists only in storing the feature vectors and class labels of the training samples. This also means that the training stage is pretty fast, since data storage is all that is actually happening. The training step is much faster for the nearest neighbors compared to other machine learning algorithms.\n",
    "4. KNN can be used for both classification and Regression.\n",
    "The natural practice of this method is placing input points into appropriate categories, but we can easily extend this algorithm to regression problems. Instead of combining the discrete labels of k-neighbors we have to combine continuous predictions. These predictions can be obtained in different ways, for example, by averaging the values of the k-most similar instances.\n",
    "\n",
    "\n",
    "\n",
    "## Cons of K Nearest Neighbors\n",
    "1. KNN needs high memory.\n",
    "Since it KNN memorise the input data instead of learn from them, it  needs high memory to store all the data points.\n",
    "\n",
    "2. Prediction stage is costly.\n",
    "However, when we predict new data using KNN, it will take much time to search for the nearest neighbors in the entire training dataset, calculating the distance and sorting \n",
    "them, so it is computationally expensive as it searches the nearest neighbors for the new point at the prediction stage and thus the prediction stage is very costly.\n",
    "\n",
    "3. KNN is sensitive to outliers\n",
    "Besides,k-NN classifier is sensitive to outliers, accuracy is impacted by noise or irrelevant data. Its accuracy highly depends on the quality of the training data. Noise and mislabeled data, as well as outliers and overlaps between data regions of different classes, lead to less accurate classification. it is  sensitive to outliers, since a single mislabeled example dramatically changes the class boundaries. Anomalies affect the method significantly, because k-NN gets all the information from the input, rather than from an algorithm that tries to generalize data. This problem can be dealt with by adopting either a large k value or by pre-processing the training set with an editing algorithm.  \n",
    "\n",
    "4. KNN is not suitable for high dimensional data.\n",
    "When features vector dimension is relatively high(a dataset in which the number of features p is larger than the number of observations N), k-NN might suffer. This is due to the fact that distance measures often lose accuracy: There is a little difference between the nearest and the farthest neighbors. Besides, irrelevant features and redundant features \n",
    "can affect its performance. we could use dimensionality reduction techniques (feature selection and feature extraction, like PCA) to redefine the features vector before using it as the input of the algorithm.\n",
    "\n",
    "## Time and Space Complexity Analysis\n",
    "n: number of points in the training dataset\n",
    "d: data dimensionality\n",
    "K: number of neighbours that we consider for voting\n",
    "- Traning time complexity is O(1): The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples. Since  all computation is done during prediction, so KNN has O(1) time complexity.\n",
    "- Traning space complexity is O(n*d): it needs to storage so the space occupied is O(n*d)\n",
    "- Prediction time complexity:O(k*n*d): O(d) to compute sistance to one example, o(n*d) to find one nearest neighbor, O(k*n*d) to find k closet examples, thus the complexity is O(k*n*d)\n",
    "- Prediction space complexity:O(1): Find the most frequent label in the list set just needs contant space\n",
    "\n",
    "\n",
    "# improvements to reduce complexity of KNN\n",
    "## dimensionality reduction\n",
    "- feature selection\n",
    "- pca\n",
    "## use efficient method to find nearst neighbot\n",
    "- In contrast with the fast training stage, it requires very expensive testing. All the cost of the algorithm is in the computation of the prediction, since for every test sample, the model has to run through the entire data set to compute distances and then find the nearest neighbors.\n",
    "\n",
    "Proposal: To deal with the challenging computation of k-NN, some alternative techniques to the brute-force have been created. K-D tree and Ball tree are the most well-known ones.\n",
    "## preprocessiong\n",
    "normalized features\n",
    "### How KNN works(Classification)\n",
    "KNN algorithms can be summarised in the following steps:\n",
    "\n",
    "1. Select the number k of the nearest neighbours\n",
    "2. Calculate the distance between new data point and each of the training data.\n",
    "3. Find the k nearest neighbours from the new data\n",
    "4. Take the most frequent of labels as the prediction of the label of new data point\n",
    "\n",
    "KNN algorithm can be used for both classification and regression problems. \n",
    "\n",
    "### How KNN predits\n",
    "For a given new data point, find the k nearest neighbours from new data point in traning dataset, collect neighbours' labels, finally, take the mode of the labels as the prediction for the label of new data point.\n",
    "\n",
    "### KNN Hyperparameters\n",
    "Hyperparameters are the parameters to control the model learning process. We can tune these hyperparameters to get a model with optimal performance.\n",
    "Here are part of hyperparmeters that I used in the following task.\n",
    "\n",
    "- Number of neighbors(K):the number of nearest neighbors\n",
    "- Weights: When calculating the distance, we can choose 'uniform’ weights that all points in each neighborhood are weighted equally or\n",
    "‘distance’ weight points by the inverse of their distance.\n",
    "- p: This determines that we can use what kind of metric to calculate distance between neighbours(Euclidean Distance, Manhattan Distance, and Minkowski Distance) When p = 1, use manhattan_distance, and euclidean_distance  p = 2. For arbitrary p, minkowski_distance is used.\n",
    "\n",
    "### Application\n",
    "KNN can be used in both solve both classification and regression problems, when using KNN to solve regression problem, it will use the average of target value of K nearst neighbors as the prediction value of the new data, when using to solve classification problem, it will use the mode as the prediction label of new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Here is the introduction of the dataset. \n",
    "\n",
    "This dataset is about 11 clinical features for predicting heart disease events, from Kaggle [Heart Failure Prediction Dataset](https://www.kaggle.com/fedesoriano/heart-failure-prediction/)\n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "- Age: (Discreate variable)    age of the patient [years]\n",
    "- Sex: (Binary variable)   sex of the patient [M: Male, F: Female]\n",
    "- ChestPainType: (Categorical variable) chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n",
    "- RestingBP:(Continuous variable)  resting blood pressure [mm Hg]\n",
    "- Cholesterol:(Continuous variable)    serum cholesterol [mm/dl]\n",
    "- FastingBS: (Binary variable)     fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n",
    "- RestingECG: (Categorical variable)   resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality, LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n",
    "- MaxHR:(Continuos variable)   maximum heart rate achieved [Numeric value between 60 and 202]\n",
    "- ExerciseAngina: (Binary variable)    exercise-induced angina [Y: Yes, N: No]\n",
    "- Oldpeak: (Continuous variable)   Numeric value measured in depression\n",
    "- ST_Slope: (Categorical variable)     he slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n",
    "- HeartDisease:(Binary variable)   output class [1: heart disease, 0: Normal]\n",
    "\n",
    "The classifaction goal is to predict heart falure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"heart.csv\", sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 918 observations and 12 features (11 independent features and 1 dependent feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in these dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age               0\n",
      "Sex               0\n",
      "ChestPainType     0\n",
      "RestingBP         0\n",
      "Cholesterol       0\n",
      "FastingBS         0\n",
      "RestingECG        0\n",
      "MaxHR             0\n",
      "ExerciseAngina    0\n",
      "Oldpeak           0\n",
      "ST_Slope          0\n",
      "HeartDisease      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target class distribution is relatively balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.553377\n",
       "0    0.446623\n",
       "Name: HeartDisease, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,-1].value_counts()/len(df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into Feature vectors (X) and labels (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>918.0</td>\n",
       "      <td>53.510893</td>\n",
       "      <td>9.432617</td>\n",
       "      <td>28.0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RestingBP</th>\n",
       "      <td>918.0</td>\n",
       "      <td>132.396514</td>\n",
       "      <td>18.514154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.00</td>\n",
       "      <td>130.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cholesterol</th>\n",
       "      <td>918.0</td>\n",
       "      <td>198.799564</td>\n",
       "      <td>109.384145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.25</td>\n",
       "      <td>223.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FastingBS</th>\n",
       "      <td>918.0</td>\n",
       "      <td>0.233115</td>\n",
       "      <td>0.423046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxHR</th>\n",
       "      <td>918.0</td>\n",
       "      <td>136.809368</td>\n",
       "      <td>25.460334</td>\n",
       "      <td>60.0</td>\n",
       "      <td>120.00</td>\n",
       "      <td>138.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oldpeak</th>\n",
       "      <td>918.0</td>\n",
       "      <td>0.887364</td>\n",
       "      <td>1.066570</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HeartDisease</th>\n",
       "      <td>918.0</td>\n",
       "      <td>0.553377</td>\n",
       "      <td>0.497414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count        mean         std   min     25%    50%    75%    max\n",
       "Age           918.0   53.510893    9.432617  28.0   47.00   54.0   60.0   77.0\n",
       "RestingBP     918.0  132.396514   18.514154   0.0  120.00  130.0  140.0  200.0\n",
       "Cholesterol   918.0  198.799564  109.384145   0.0  173.25  223.0  267.0  603.0\n",
       "FastingBS     918.0    0.233115    0.423046   0.0    0.00    0.0    0.0    1.0\n",
       "MaxHR         918.0  136.809368   25.460334  60.0  120.00  138.0  156.0  202.0\n",
       "Oldpeak       918.0    0.887364    1.066570  -2.6    0.00    0.6    1.5    6.2\n",
       "HeartDisease  918.0    0.553377    0.497414   0.0    0.00    1.0    1.0    1.0"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical values\n",
    "The dataset contains categorical variables like 'ChestPainType'\n",
    "\n",
    "Since meachine learning algorithms cannot work with categorical data directly, we need to convert the categorical values into numeric values by using one-hot code.\n",
    "\n",
    "To handle the categorical variables we have to create dummy variables out of a categorical variable and include them instead of the original categorical variable. Unlike regression, create k dummies instead of (k-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>ChestPainType_ASY</th>\n",
       "      <th>ChestPainType_ATA</th>\n",
       "      <th>ChestPainType_NAP</th>\n",
       "      <th>ChestPainType_TA</th>\n",
       "      <th>RestingECG_LVH</th>\n",
       "      <th>RestingECG_Normal</th>\n",
       "      <th>RestingECG_ST</th>\n",
       "      <th>ExerciseAngina_N</th>\n",
       "      <th>ExerciseAngina_Y</th>\n",
       "      <th>ST_Slope_Down</th>\n",
       "      <th>ST_Slope_Flat</th>\n",
       "      <th>ST_Slope_Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>160</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>130</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48</td>\n",
       "      <td>138</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>150</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  RestingBP  Cholesterol  FastingBS  MaxHR  Oldpeak  Sex_F  Sex_M  \\\n",
       "0   40        140          289          0    172      0.0      0      1   \n",
       "1   49        160          180          0    156      1.0      1      0   \n",
       "2   37        130          283          0     98      0.0      0      1   \n",
       "3   48        138          214          0    108      1.5      1      0   \n",
       "4   54        150          195          0    122      0.0      0      1   \n",
       "\n",
       "   ChestPainType_ASY  ChestPainType_ATA  ChestPainType_NAP  ChestPainType_TA  \\\n",
       "0                  0                  1                  0                 0   \n",
       "1                  0                  0                  1                 0   \n",
       "2                  0                  1                  0                 0   \n",
       "3                  1                  0                  0                 0   \n",
       "4                  0                  0                  1                 0   \n",
       "\n",
       "   RestingECG_LVH  RestingECG_Normal  RestingECG_ST  ExerciseAngina_N  \\\n",
       "0               0                  1              0                 1   \n",
       "1               0                  1              0                 1   \n",
       "2               0                  0              1                 1   \n",
       "3               0                  1              0                 0   \n",
       "4               0                  1              0                 1   \n",
       "\n",
       "   ExerciseAngina_Y  ST_Slope_Down  ST_Slope_Flat  ST_Slope_Up  \n",
       "0                 0              0              0            1  \n",
       "1                 0              0              1            0  \n",
       "2                 0              0              0            1  \n",
       "3                 1              0              1            0  \n",
       "4                 0              0              0            1  "
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying One hot Encoding on the Categorical columns\n",
    "X=pd.get_dummies(X)\n",
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don’t want the model to over-learn from training data and perform poorly after being tested in test data. we need to assess how well the model is generalizing. Hence, so we  separate input data set into training, validation, and testing subsets to prevent the model from overfitting and to evaluate the model effectively.\n",
    "\n",
    "Train dataset is used to fit the parameters to the model, or 'finding nearest neighbors'\n",
    "\n",
    "Valid dataset is used to assess the model's performance and avoid overfitting (the model is really good at classifying the samples in the training dataset but cannot generalize and make accurate classifications on the unseen data), it helps us tune the model's hyperparameters, so we can improve the model according to the results on validation.\n",
    "\n",
    "test dataset is used to assess the performance of the final chosen model and it is equivalent to the future unseen data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset \n",
    "we split dataset into train dataset, validation dataset, and test dataset in the ratio 8:1:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train dataset 80%, valid dataset 10%, test dataset 10%\n",
    "# split the data in training and remaining dataset, and split the remaining dataset\n",
    "X_remain, X_test, y_remain, y_test = train_test_split(X, y, test_size = 0.1, random_state = 3)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_remain, y_remain, test_size = 1/9,random_state = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data is important for kNN as bringing all the features to the same scale is recommended for applying distance-based algorithms. If the scale of features is very different then normalization is required. This is because the distance calculation done in KNN uses feature values. When the one feature values are large than other, that feature will dominate the distance hence the outcome of the KNN. In this dataset, the 'Age' attribute is from 28 to 77, the 'Cholesterol' is from 0 to 603. Since both the features have different scales, there is a chance that higher weightage is given to features with higher magnitude. This will impact the performance of the machine learning algorithm and obviously, we do not want our algorithm to be biassed towards one feature like 'Cholesterol' in this case.\n",
    "\n",
    "Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result, we need to scale our dataset.\n",
    "\n",
    "I use Data Standardization method to resclae the attributes so that they have mean as 0 and variance as 1, this can bring down all the features to a common scale without distorting the differences in the range of the values.\n",
    "\n",
    "I have imported standard scaler from scikit-learn machine learning library software. I call fit_transform() method on our training and validation dataset and transform() method on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "Choosing the right metric is crucial while evaluating machine learning models. \n",
    "\n",
    "Accuracy is an intuitive metric to assess the perfomance of model, and is defined as the number of correct predictions divided by the total number of predictions, multiplied by 100.\n",
    "\n",
    "accuracy is applied in when the target class distribution is fairly balanced and we do not prefer any type of class. So I decide to use 'accuracy_score' function for this classication task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train KNN model\n",
    "\n",
    "I use 'KNeighborsClassifier' function form 'sklearn' library to generate KNN model.\n",
    "First, I use train dataset to fit the KNN model and iterate the hyperparameters that we want to tuning (p, k, weigth) and in each for loop tries a combination of hyperparameters. The model tuning the hyperparameters based on it's perfomance (assessed by classification accuracy score) on validation data set, and finally we record the best hyperparameters for the training model.\n",
    "\n",
    "### Hyperparameters tuning\n",
    "p parameter for the distance metric. When p = 1, this is equivalent to using manhattan_distance, and euclidean_distance for p = 2. For arbitrary p (eg. p=3), minkowski_distance is used.\n",
    "\n",
    "weights{‘uniform’, ‘distance’}\n",
    "Weight function used in prediction. Possible values:\n",
    "‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_p= 1\n",
      "best k= 25\n",
      "best weight= uniform\n",
      "best score= 0.7934782608695652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "best_p = 1\n",
    "best_score = 0.0\n",
    "best_k = 1\n",
    "weight_option = [\"uniform\", \"distance\"]\n",
    "k_range = list(range(3,31,2))\n",
    "p_options = [1,2,3]\n",
    "\n",
    "for p_o in p_options:\n",
    "    for k in k_range:\n",
    "        for weight in weight_option:\n",
    "            knn = KNeighborsClassifier(n_neighbors = k, weights = weight, p = p_o)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_valid_pred = knn.predict(X_valid)\n",
    "            score = knn.score(X_valid, y_valid) # Return the mean accuracy on the validate dataset\n",
    "            if score > best_score:\n",
    "                best_p = p_o\n",
    "                best_k = k\n",
    "                best_weight = weight\n",
    "                best_score = score\n",
    "                \n",
    "print(\"best_p=\", best_p)\n",
    "print(\"best k=\", best_k)\n",
    "print(\"best weight=\", best_weight)\n",
    "print(\"best score=\", best_score)\n",
    "print('Test set accuracy: %.2f%%' % accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test KNN model on test dataset\n",
    "\n",
    "I get 86.96% prediction accuracy on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.8696\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 25, p = 1, weights= 'uniform')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred=knn.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.4f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After use Standaization, The accuracy increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train dataset 80%, valid dataset 10%, test dataset 10%\n",
    "# split the data in training and remaining dataset, and split the remaining dataset\n",
    "X_remain, X_test, y_remain, y_test = train_test_split(X, y, test_size = 0.1, random_state = 3)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_remain, y_remain, test_size = 1/9,random_state = 3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x=StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_valid = sc_x.fit_transform(X_valid)\n",
    "X_test=sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_p= 1\n",
      "best k= 9\n",
      "best weight= uniform\n",
      "best score= 0.9021739130434783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "best_p = 1\n",
    "best_score = 0.0\n",
    "best_k = 1\n",
    "weight_option = [\"uniform\", \"distance\"]\n",
    "k_range = list(range(3,31,2))\n",
    "p_options = [1,2,3]\n",
    "\n",
    "for p_o in p_options:\n",
    "    for k in k_range:\n",
    "        for weight in weight_option:\n",
    "            knn = KNeighborsClassifier(n_neighbors = k, weights = weight, p = p_o)\n",
    "            knn.fit(X_train, y_train)\n",
    "            y_valid_pred = knn.predict(X_valid)\n",
    "            score = knn.score(X_valid, y_valid) # Return the mean accuracy on the validate dataset\n",
    "            if score > best_score:\n",
    "                best_p = p_o\n",
    "                best_k = k\n",
    "                best_weight = weight\n",
    "                best_score = score\n",
    "                \n",
    "print(\"best_p=\", best_p)\n",
    "print(\"best k=\", best_k)\n",
    "print(\"best weight=\", best_weight)\n",
    "print(\"best score=\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.9130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 9, p = 1, weights= 'uniform')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred=knn.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.4f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Now, let's try use cross-validation to tune the hyperparameters.\n",
    "\n",
    "There are a lot of different techniques that may be used to cross-validate a model. In this task, I choose K-folds validation to conduct cross-validation and set k = 5, here is the how it works.\n",
    "\n",
    "Dataset is randomly split up into 'k' folds. One of the folds is validation set and the rest are training set. The model is trained on the training set and scored on the validation set to find best hyperparameters. Then the process is repeated until each group has been used as the validation set.\n",
    "\n",
    "### Train KNN_2 model\n",
    "I use 'GridSearchCV' function from 'sklearn' library to find the best set of hyperparameters that I tuning(k,p,distance).\n",
    "There are four parameters I specify in this function: estimator ,param_grid,scoring,cv. I will explain how it use.\n",
    "\n",
    "estimator:  the model that I train\n",
    "param_grid: In the grid search method I specified with the 'param_grid‘ grid parameter with possible values for hyperparameters.when “fitting” it on a dataset all the possible combinations of parameter values are evaluated records the model performance (accuracy score). Finally, it returns the best KNN model with the best hyperparameters.I use accuracy score to choose the best hyperparameters combination.\n",
    "Scoring: I choose accuracy score to access the model performance.  \n",
    "\n",
    "CV: Strategy to evaluate the performance of the cross-validated model on the test set. I choose 5-fold cross validation as cross-validation splitting strategy. Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train dataset 80%, valid dataset 10%, test dataset 10%\n",
    "# split the data in training and remaining dataset, and split the remaining dataset\n",
    "X_remain, X_test, y_remain, y_test = train_test_split(X, y, test_size = 0.1, random_state = 3)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_remain, y_remain, test_size = 1/9,random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters {'n_neighbors': 17, 'p': 1, 'weights': 'distance'}\n",
      "accuracy_score : 0.7748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters that we want to tune.\n",
    "k_range = list(range(3,31,2))\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "p_options=[1,2,3]\n",
    "\n",
    "param_dict = dict(n_neighbors = k_range, weights = weight_options, p = p_options)\n",
    "\n",
    "#Create KNN classiifier\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use 5 fold cross-validation, use accuracy to evaluate the performance of the model on the validation set\n",
    "grid = GridSearchCV(estimator = knn_2, param_grid = param_dict, scoring = 'accuracy', cv = 5) \n",
    "\n",
    "#Fit the model (remain dataset includes train and validation dataset that I split before.)\n",
    "best_model = grid.fit(X_remain,y_remain) \n",
    "\n",
    "#Print the value of best hyperparameters and best accuracy score\n",
    "print('best hyperparameters', best_model.best_params_)\n",
    "print('accuracy_score : %.4f' % best_model.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.934783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_2 = KNeighborsClassifier(n_neighbors = 17, p = 1, weights= 'distance')\n",
    "\n",
    "knn_2.fit(X_remain, y_remain)\n",
    "\n",
    "y_test_pred=knn_2.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.6f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After use standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x=StandardScaler()\n",
    "X_remain = sc_x.fit_transform(X_remain)\n",
    "X_test=sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters {'n_neighbors': 29, 'p': 1, 'weights': 'distance'}\n",
      "accuracy_score : 0.8716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters that we want to tune.\n",
    "k_range = list(range(3,31,2))\n",
    "weight_options = [\"distance\",\"uniform\"]\n",
    "p_options=[1,2,3]\n",
    "\n",
    "param_dict = dict(n_neighbors = k_range, weights = weight_options, p = p_options)\n",
    "\n",
    "#Create KNN classiifier\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use 5 fold cross-validation, use accuracy to evaluate the performance of the model on the validation set\n",
    "grid = GridSearchCV(knn_2, param_grid = param_dict, scoring = 'accuracy', cv = 5) \n",
    "\n",
    "#Fit the model (remain dataset includes train and validation dataset that I split before.)\n",
    "best_model = grid.fit(X_remain,y_remain) \n",
    "\n",
    "#Print the value of best hyperparameters and best accuracy score\n",
    "print('best hyperparameters', best_model.best_params_)\n",
    "print('accuracy_score : %.4f' % best_model.best_score_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test KNN_2 model on test dataset\n",
    "\n",
    "After tuning the hyperparameters by crossvalidation, I get another model with different hyperparameters, then I test the best performance model on test dataset and get 88.04% prediction accuracy, which is better than before(86.96%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy : 0.945652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_2 = KNeighborsClassifier(n_neighbors = 29, p = 1, weights= 'distance')\n",
    "\n",
    "knn_2.fit(X_remain, y_remain)\n",
    "\n",
    "y_test_pred=knn_2.predict(X_test) \n",
    "\n",
    "test_accuracy = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print('test_accuracy : %.6f'%test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model that trained by cross validation has a higher accuracy score than the first model on the same test dataset. \n",
    "That is because when splitting the origin dataset into three sets, which reduce the amount of data in training date which can be used for learning the model, and the results can depend on a particular random choice for the pair of (training, validation) sets. But when useing cross validation, we can use of training data more effiently as every observation is used for both training and validating, thus get more accurate estimate of unseen data. \n",
    "However, it takes 13.8s to run cross-validation and it takes only 3.2 to run on one validation dataset, because K-fold cross-validation repeats the train/test split K-times, so it need more time to train the model and tune hyperparameters.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d4b19282c4f1a35886decbca8caed4edeb52dcc9663281933ed3bb3d17a686d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
